{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-04T23:33:41.404645Z",
     "start_time": "2024-12-04T23:33:40.815722Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. What is Machine Learning?\n",
    "\n",
    "Machine Learning (ML) is a subset of artificial intelligence that focuses on developing systems that can learn from and make decisions based on data. Unlike traditional programming where we explicitly write rules, ML algorithms learn patterns from data to make predictions or decisions.\n",
    "\n",
    "## Learning in General\n",
    "\n",
    "Learning can be defined as the process of improving performance on a specific task through experience. In ML, this translates to:\n",
    "- **Experience**: Training data\n",
    "- **Task**: The problem we're trying to solve (prediction, classification, etc.)\n",
    "- **Performance**: How well our model performs (measured by metrics)\n",
    "\n",
    "The fundamental goal is to learn patterns that generalize well to unseen data."
   ],
   "id": "e47a45140cc2393f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Types of Machine Learning\n",
    "\n",
    "## 2.1 Supervised Learning\n",
    "- Learning from labeled data\n",
    "- Goal: Learn a mapping from inputs to outputs\n",
    "- Examples: Classification, Regression\n",
    "\n",
    "## 2.2 Unsupervised Learning\n",
    "- Learning from unlabeled data\n",
    "- Goal: Find hidden patterns or structures in data\n",
    "- Examples: Clustering, Dimensionality Reduction\n",
    "\n",
    "## 2.3 Reinforcement Learning\n",
    "- Learning through interaction with an environment\n",
    "- Goal: Learn optimal actions to maximize rewards\n",
    "- Examples: Game playing, Robot navigation\n"
   ],
   "id": "7f3c828956b7e1bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. The Learning Problem\n",
    "\n",
    "The fundamental learning problem can be formulated as follows:\n",
    "\n",
    "Given:\n",
    "- A task $T$\n",
    "- A performance measure $P$\n",
    "- Training experience $E$\n",
    "\n",
    "Find:\n",
    "- A function $f: X \\rightarrow Y$ that optimizes $P$ on $T$\n",
    "\n",
    "\n",
    "Mathematically, we can express this as:\n",
    "\n",
    "$f^* = \\text{argmin}_f \\mathbb{E}_{(x,y)\\sim D}[L(f(x), y)]$\n",
    "\n",
    "Where:\n",
    "- $f^*$ is the optimal function\n",
    "- $D$ is the true data distribution\n",
    "- $L$ is the loss function\n",
    "- $x$ is the input\n",
    "- $y$ is the true output"
   ],
   "id": "9ea6a38a62d18beb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Basic Terminology\n",
    "\n",
    "## 4.1 Key Terms\n",
    "- **Features (X)**: Input variables used for prediction\n",
    "- **Labels (y)**: Output variables we're trying to predict\n",
    "- **Model**: The function we learn from data\n",
    "- **Parameters**: Values learned during training\n",
    "- **Hyperparameters**: Configuration values set before training\n",
    "- **Training**: Process of learning from data\n",
    "- **Inference**: Using the trained model to make predictions"
   ],
   "id": "8c71104e46a920fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Training vs Testing\n",
    "\n",
    "One of the most fundamental concepts in machine learning is the division between training and testing data:\n",
    "\n",
    "- **Training Data**: Used to learn the model parameters\n",
    "- **Testing Data**: Used to evaluate model performance on unseen data\n",
    "\n",
    "The key principle is that we must evaluate our model on data it hasn't seen during training to get an honest estimate of its performance.\n"
   ],
   "id": "bb658feb5513a85a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. Generalization\n",
    "\n",
    "Generalization refers to a model's ability to perform well on unseen data. It's the ultimate goal of machine learning.\n",
    "\n",
    "## Mathematical Definition\n",
    "\n",
    "The generalization error (or true error) is defined as:\n",
    "\n",
    "$$E_{out} = \\mathbb{E}_{(x,y)\\sim D}[L(h(x), y)]$$\n",
    "\n",
    "Where:\n",
    "- $$E_{out}$$ is the generalization error\n",
    "- $$D$$ is the true data distribution\n",
    "- $$L$$ is the loss function\n",
    "- $$h$$ is our hypothesis (model)\n",
    "\n",
    "The empirical error (training error) is:\n",
    "\n",
    "$$E_{in} = \\frac{1}{n}\\sum_{i=1}^n L(h(x_i), y_i)$$\n",
    "\n",
    "The goal is to minimize $$E_{out}$$, but we can only observe $$E_{in}$$."
   ],
   "id": "d6115ebae573528e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:15:36.130451Z",
     "start_time": "2024-12-05T12:15:35.517906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Week 1: Introduction to Machine Learning\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(42)\n",
    "\n",
    "# %% [markdown]\n",
    "# # 1. What is Machine Learning?\n",
    "\n",
    "# Machine Learning (ML) is a subset of artificial intelligence that focuses on developing systems that can learn from and make decisions based on data. Unlike traditional programming where we explicitly write rules, ML algorithms learn patterns from data to make predictions or decisions.\n",
    "\n",
    "# ## Learning in General\n",
    "\n",
    "# Learning can be defined as the process of improving performance on a specific task through experience. In ML, this translates to:\n",
    "# - **Experience**: Training data\n",
    "# - **Task**: The problem we're trying to solve (prediction, classification, etc.)\n",
    "# - **Performance**: How well our model performs (measured by metrics)\n",
    "\n",
    "# The fundamental goal is to learn patterns that generalize well to unseen data.\n",
    "\n",
    "# %% [markdown]\n",
    "# # 2. Types of Machine Learning\n",
    "\n",
    "# ## 2.1 Supervised Learning\n",
    "# - Learning from labeled data\n",
    "# - Goal: Learn a mapping from inputs to outputs\n",
    "# - Examples: Classification, Regression\n",
    "\n",
    "# ## 2.2 Unsupervised Learning\n",
    "# - Learning from unlabeled data\n",
    "# - Goal: Find hidden patterns or structures in data\n",
    "# - Examples: Clustering, Dimensionality Reduction\n",
    "\n",
    "# ## 2.3 Reinforcement Learning\n",
    "# - Learning through interaction with an environment\n",
    "# - Goal: Learn optimal actions to maximize rewards\n",
    "# - Examples: Game playing, Robot navigation\n",
    "\n",
    "# Let's visualize these different types with simple examples:\n",
    "\n",
    "# %%\n",
    "# Create example data for visualization\n",
    "def create_example_data():\n",
    "    # Supervised Learning Example\n",
    "    X_supervised = np.random.normal(size=(100, 1))\n",
    "    y_supervised = 2 * X_supervised + 1 + np.random.normal(scale=0.3, size=(100, 1))\n",
    "\n",
    "    # Unsupervised Learning Example\n",
    "    cluster1 = np.random.normal(loc=(-2, -2), scale=0.5, size=(50, 2))\n",
    "    cluster2 = np.random.normal(loc=(2, 2), scale=0.5, size=(50, 2))\n",
    "    X_unsupervised = np.vstack([cluster1, cluster2])\n",
    "\n",
    "    return X_supervised, y_supervised, X_unsupervised\n",
    "\n",
    "X_sup, y_sup, X_unsup = create_example_data()\n",
    "\n",
    "# Plotting\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Supervised Learning Plot\n",
    "ax1.scatter(X_sup, y_sup, alpha=0.5)\n",
    "ax1.set_title('Supervised Learning\\n(Regression Example)')\n",
    "ax1.set_xlabel('Input Feature (X)')\n",
    "ax1.set_ylabel('Target Variable (y)')\n",
    "\n",
    "# Unsupervised Learning Plot\n",
    "ax2.scatter(X_unsup[:, 0], X_unsup[:, 1], alpha=0.5)\n",
    "ax2.set_title('Unsupervised Learning\\n(Clustering Example)')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# # 3. The Learning Problem\n",
    "\n",
    "# The fundamental learning problem can be formulated as follows:\n",
    "\n",
    "# Given:\n",
    "# - A task $$T$$\n",
    "# - A performance measure $$P$$\n",
    "# - Training experience $$E$$\n",
    "\n",
    "# Find:\n",
    "# - A function $$f: X \\rightarrow Y$$ that optimizes $$P$$ on $$T$$\n",
    "\n",
    "# Mathematically, we can express this as:\n",
    "\n",
    "# $$f^* = \\argmin_f \\mathbb{E}_{(x,y)\\sim D}[L(f(x), y)]$$\n",
    "\n",
    "# Where:\n",
    "# - $$f^*$$ is the optimal function\n",
    "# - $$D$$ is the true data distribution\n",
    "# - $$L$$ is the loss function\n",
    "# - $$x$$ is the input\n",
    "# - $$y$$ is the true output\n",
    "\n",
    "# %% [markdown]\n",
    "# # 4. Basic Terminology\n",
    "\n",
    "# ## 4.1 Key Terms\n",
    "# - **Features (X)**: Input variables used for prediction\n",
    "# - **Labels (y)**: Output variables we're trying to predict\n",
    "# - **Model**: The function we learn from data\n",
    "# - **Parameters**: Values learned during training\n",
    "# - **Hyperparameters**: Configuration values set before training\n",
    "# - **Training**: Process of learning from data\n",
    "# - **Inference**: Using the trained model to make predictions\n",
    "\n",
    "# Let's visualize these concepts with a simple dataset:\n",
    "\n",
    "# %%\n",
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = np.sin(X) + np.random.normal(0, 0.1, size=X.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.5, label='Data points')\n",
    "plt.xlabel('Feature (X)')\n",
    "plt.ylabel('Label (y)')\n",
    "plt.title('Example Dataset: Features and Labels')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# # 5. Training vs Testing\n",
    "\n",
    "# One of the most fundamental concepts in machine learning is the division between training and testing data:\n",
    "\n",
    "# - **Training Data**: Used to learn the model parameters\n",
    "# - **Testing Data**: Used to evaluate model performance on unseen data\n",
    "\n",
    "# The key principle is that we must evaluate our model on data it hasn't seen during training to get an honest estimate of its performance.\n",
    "\n",
    "# Let's demonstrate this concept:\n",
    "\n",
    "# %%\n",
    "def create_nonlinear_data(n_samples=100, noise=0.1):\n",
    "    X = np.linspace(-3, 3, n_samples).reshape(-1, 1)\n",
    "    y = np.sin(X) + np.random.normal(0, noise, size=X.shape)\n",
    "    return X, y\n",
    "\n",
    "# Create and split data\n",
    "X, y = create_nonlinear_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a polynomial regression\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train_poly)\n",
    "y_test_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, alpha=0.5, label='Training data')\n",
    "plt.plot(X_train, y_train_pred, 'r-', label='Model predictions')\n",
    "plt.title('Training Set')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, alpha=0.5, label='Test data')\n",
    "plt.plot(X_test, y_test_pred, 'r-', label='Model predictions')\n",
    "plt.title('Test Set')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# # 6. Generalization\n",
    "\n",
    "# Generalization refers to a model's ability to perform well on unseen data. It's the ultimate goal of machine learning.\n",
    "\n",
    "# ## Mathematical Definition\n",
    "\n",
    "# The generalization error (or true error) is defined as:\n",
    "\n",
    "# $$E_{out} = \\mathbb{E}_{(x,y)\\sim D}[L(h(x), y)]$$\n",
    "\n",
    "# Where:\n",
    "# - $$E_{out}$$ is the generalization error\n",
    "# - $$D$$ is the true data distribution\n",
    "# - $$L$$ is the loss function\n",
    "# - $$h$$ is our hypothesis (model)\n",
    "\n",
    "# The empirical error (training error) is:\n",
    "\n",
    "# $$E_{in} = \\frac{1}{n}\\sum_{i=1}^n L(h(x_i), y_i)$$\n",
    "\n",
    "# The goal is to minimize $$E_{out}$$, but we can only observe $$E_{in}$$.\n",
    "\n",
    "# %% [markdown]\n",
    "# # 7. Bias-Variance Tradeoff\n",
    "\n",
    "# The bias-variance tradeoff is a fundamental concept in machine learning that helps us understand prediction error.\n",
    "\n",
    "# ## Mathematical Decomposition\n",
    "\n",
    "# For a given point x, the expected prediction error can be decomposed as:\n",
    "\n",
    "# $$\\mathbb{E}[(y - \\hat{f}(x))^2] = \\text{Bias}[\\hat{f}(x)]^2 + \\text{Variance}[\\hat{f}(x)] + \\sigma^2$$\n",
    "\n",
    "# Where:\n",
    "# - **Bias**: Error from wrong assumptions\n",
    "# - **Variance**: Error from sensitivity to training data\n",
    "# - $$\\sigma^2$$: Irreducible error\n",
    "\n",
    "# Let's visualize this tradeoff:\n",
    "\n",
    "# %%\n",
    "def generate_bias_variance_data():\n",
    "    # Generate true function\n",
    "    X = np.linspace(0, 10, 100)\n",
    "    y_true = np.sin(X)\n",
    "\n",
    "    # Generate training data points\n",
    "    X_train = np.random.uniform(0, 10, 20)\n",
    "    y_train = np.sin(X_train) + np.random.normal(0, 0.1, 20)\n",
    "\n",
    "    return X, y_true, X_train, y_train\n",
    "\n",
    "X, y_true, X_train, y_train = generate_bias_variance_data()\n",
    "\n",
    "# Fit models with different complexities\n",
    "degrees = [1, 5, 15]  # Different polynomial degrees\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, degree in enumerate(degrees, 1):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
    "    X_train_poly = poly.transform(X_train.reshape(-1, 1))\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_poly)\n",
    "\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.scatter(X_train, y_train, color='blue', alpha=0.5, label='Training data')\n",
    "    plt.plot(X, y_true, 'g-', label='True function', alpha=0.5)\n",
    "    plt.plot(X, y_pred, 'r-', label=f'Polynomial (degree={degree})')\n",
    "    plt.title(f'Degree {degree} Polynomial')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# # 8. Overfitting and Underfitting\n",
    "\n",
    "# - **Underfitting**: Model is too simple to capture the underlying pattern (high bias)\n",
    "# - **Overfitting**: Model is too complex and captures noise in the training data (high variance)\n",
    "# - **Just right**: Model captures the underlying pattern without fitting the noise\n",
    "\n",
    "# Let's visualize these concepts:\n",
    "\n",
    "# %%\n",
    "def generate_overfit_data(n_samples=50):\n",
    "    X = np.linspace(0, 10, n_samples)\n",
    "    y_true = np.sin(X)\n",
    "    y = y_true + np.random.normal(0, 0.1, n_samples)\n",
    "    return X, y, y_true\n",
    "\n",
    "X, y, y_true = generate_overfit_data()\n",
    "\n",
    "# Create models with different complexities\n",
    "degrees = [1, 3, 15]\n",
    "titles = ['Underfitting', 'Good Fit', 'Overfitting']\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, (degree, title) in enumerate(zip(degrees, titles), 1):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "\n",
    "    # Generate smooth curve for predictions\n",
    "    X_smooth = np.linspace(0, 10, 200)\n",
    "    X_smooth_poly = poly.transform(X_smooth.reshape(-1, 1))\n",
    "    y_smooth = model.predict(X_smooth_poly)\n",
    "\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.scatter(X, y, color='blue', alpha=0.5, label='Data points')\n",
    "    plt.plot(X, y_true, 'g-', label='True function', alpha=0.5)\n",
    "    plt.plot(X_smooth, y_smooth, 'r-', label=f'Model (degree={degree})')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Key Takeaways\n",
    "\n",
    "# 1. **Model Complexity**:\n",
    "#    - Underfitting: Model is too simple (high bias)\n",
    "#    - Overfitting: Model is too complex (high variance)\n",
    "#    - Goal: Find the right balance\n",
    "\n",
    "# 2. **Signs of Overfitting**:\n",
    "#    - Very low training error\n",
    "#    - High test error\n",
    "#    - Complex, wiggly decision boundaries\n",
    "\n",
    "# 3. **Signs of Underfitting**:\n",
    "#    - High training error\n",
    "#    - High test error\n",
    "#    - Too simple to capture patterns\n",
    "\n",
    "# 4. **Prevention**:\n",
    "#    - Cross-validation\n",
    "#    - Regularization\n",
    "#    - Early stopping\n",
    "#    - More/less features\n",
    "#    - More/less data\n",
    "\n",
    "# %% [markdown]\n",
    "# # Practice Exercises\n",
    "\n",
    "# 1. Generate your own dataset and split it into training and test sets\n",
    "# 2. Fit models with different complexities and observe the results\n",
    "# 3. Calculate and plot training vs. testing error for different model complexities\n",
    "# 4. Experiment with different noise levels in the data and observe the impact on overfitting\n",
    "\n",
    "# %% [markdown]\n",
    "# # Additional Resources\n",
    "\n",
    "# 1. Caltech Learning from Data Course: https://work.caltech.edu/telecourse.html\n",
    "# 2. MIT Introduction to ML: https://openlearning.mit.edu/courses/machine-learning\n",
    "# 3. Scikit-learn Documentation: https://scikit-learn.org/stable/\n",
    "# 4. \"Understanding Machine Learning: From Theory to Algorithms\" by Shai Shalev-Shwartz and Shai Ben-David"
   ],
   "id": "66bd1c71378a838a",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinear_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LinearRegression\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'seaborn'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b228d3ae9b937bed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
